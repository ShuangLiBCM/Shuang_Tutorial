{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "* How is the model defined?\n",
    "\n",
    "$\\widehat{y} = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{n}x_{n} = \\theta^{T}X $\n",
    "\n",
    "* How is the loss defined?\n",
    "\n",
    "$MSE(X, \\theta_{h}) = \\frac {1}{m} \\Sigma (y_{i} - \\theta_{h}^{T}X_{i})^2 $\n",
    "\n",
    "* How is the model trained?\n",
    "\n",
    "-> Normal equation\n",
    "\n",
    "$ \\frac{\\partial{MSE}}{\\partial{\\theta_{h}}} = \\frac{2}{m} (y - \\theta_{h}^TX)^T (- X) = 0\n",
    "=> \\theta_{h} = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "    Computational complexity scales fast given increased number of features\n",
    "\n",
    "-> Gradient descent\n",
    "    \n",
    "    Tweak parameter interatively until the minimum of the cost function is reached.\n",
    "    - learning rate\n",
    "    - Feature scale\n",
    "    \n",
    "    Batch gradient descent -- utilize all the training examples\n",
    "$\\frac{\\partial{}}{\\partial{\\theta_{j}}}MSE(X, \\theta) = \\frac {2}{m} \\Sigma (\\theta_{h}^{T}x_{i}-y_{i})x^{i}_{j}\n",
    "= \\frac {2}{m}X^T (X\\theta-y)$ -- Gradient vector of the cost function\n",
    "\n",
    "    Gradient vector always point up hill, when update the parameter, take the downhill direction\n",
    "    \n",
    "$\\theta = \\theta - \\eta \\nabla {MSE}_{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized linear model\n",
    "\n",
    "Ridge regression -- sensitive to feature scaling\n",
    "\n",
    "$ J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\Sigma \\theta_i^2$\n",
    "\n",
    "Lasso regression\n",
    "\n",
    "$ J(\\theta) = MSE(\\theta) + \\alpha \\Sigma |\\theta_{i}|$\n",
    "\n",
    "Elastic Nets\n",
    "\n",
    "$ J(\\theta) = MSE(\\theta) + r\\alpha \\Sigma {|\\theta_{i}|} + \\frac {1-r}{2} \\alpha \\Sigma{\\theta_i}^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "* How is the model defined?\n",
    "\n",
    "Estimate the probability that an instance belongs to a particular class. Instead of outputing linear regression result, it outputs the logistic of the result\n",
    "\n",
    "$\\widehat p = \\sigma (\\theta^T x)$ \n",
    "\n",
    "where the logistic/logit function: $\\sigma(t) = \\frac {1}{1+exp(-t)}$\n",
    "\n",
    "* How is the loss (cost function) defined?\n",
    "\n",
    "$J(\\theta) = -\\frac{1}{m}\\Sigma y^{i}log(\\widehat {p}_{i}) + (1-y^i)log(1-\\widehat p_{i})$\n",
    "\n",
    "* How is the model trained?\n",
    "\n",
    "$ \\frac{\\partial{}}{\\partial{\\theta_{j}}}J(\\theta) = \\frac {1}{m} \\Sigma (\\sigma(\\theta_{h}^{T}x_{i})-y_{i})x^{i}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\n",
    "* How is the model defined?\n",
    "\n",
    "For logistic regression, the weight matrices is n * 1, where n is the feature size +1. For softmax regression, the weight matrices is n * k, where k is the number of output classes\n",
    "\n",
    "$ S_{k}(x) = \\theta^T_{k}x$\n",
    "\n",
    "* How is the loss (cost function) defined?  -- crossentropy\n",
    "\n",
    "$\\widehat p_{k} = \\frac {exp(s_k(x))}{\\Sigma exp(s_k(x))}$\n",
    "\n",
    "$J(\\theta) = -\\frac{1}{m}\\Sigma_{m} \\Sigma_{k} y_k log(\\widehat {p_k})$\n",
    "\n",
    "(cross entropy between p(x) and q(x): $H(p, q) = - \\Sigma p(x)log(q(x))$\n",
    "\n",
    "* How is the model trained?\n",
    "\n",
    "$\\nabla J(\\theta)_{\\theta_{k}} = \\frac{1}{m}\\Sigma (\\widehat p^i_k - \\widehat y^i_k)x^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "Large margin classifier.\n",
    "\n",
    "    -- Hard margin classifier or soft margin classifier\n",
    "    -- Polynomial kernel methods (svm(kernel='poly', gamma=5)\n",
    "    -- Similarity feature methods (svm(kernel='rbf', gamma=5)\n",
    "\n",
    "\n",
    "* How is the model defined?\n",
    "    \n",
    "    $\\widehat y = 1, if w^Tx+b <0$\n",
    "    \n",
    "    $\\widehat y = 0, if w^Tx+b >=0 $\n",
    "\n",
    "* How is the loss (cost function) defined?\n",
    "\n",
    "    -- Hard margin\n",
    "\n",
    "    minimize $\\frac {1}{2}w^T w $\n",
    "    \n",
    "    subjective to $ t^{i}(w^Tx^i+ b) >= 1, t^{i} = 1, or -1$\n",
    "    \n",
    "    -- Soft margine\n",
    "    \n",
    "    minimize $\\frac {1}{2}w^T w + C \\Sigma \\zeta_i$\n",
    "    \n",
    "    subjective to $ t^{i}(w^Tx^i + b) >= 1-\\zeta_i, t^{i} = 1, or -1$\n",
    "    \n",
    "\n",
    "* How is the model trained?\n",
    "    \n",
    "    -- For soft and hard margin problem, use the off the shelf QP solver\n",
    "\n",
    "* Kernel trick\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How is the model defined?\n",
    "\n",
    "* How is the loss (cost function) defined?\n",
    "\n",
    "Gini impurity: $G_i = 1 - \\Sigma p_{i,k}^2$, k is the number of instances belongs to class k divided by all the instances at node i\n",
    "\n",
    "$J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$\n",
    "\n",
    "Use entropy to measure impurity\n",
    "\n",
    "$H_i = -\\Sigma p_{k,i}log (p_{k,i})$, if the group is pure, $H_i = 0$\n",
    "\n",
    "* How is the model trained?\n",
    "    - CART algorithm: create binary trees, only two children per leaf\n",
    "    - ID3: more than two Children per leaf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost and Gradient boost\n",
    "\n",
    "Adaboost: pay more attention to the training instances that the predecessor underfitted\n",
    "\n",
    "Gradientboost: fit the new predictor to residual error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How is the model defined?\n",
    "\n",
    "* How is the loss (cost function) defined?\n",
    "\n",
    "* How is the model trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How is the model defined?\n",
    "\n",
    "* How is the loss (cost function) defined?\n",
    "\n",
    "* How is the model trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How is the model defined?\n",
    "\n",
    "* How is the loss (cost function) defined?\n",
    "\n",
    "* How is the model trained? -- crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How is the model defined?\n",
    "\n",
    "* How is the loss (cost function) defined?\n",
    "\n",
    "* How is the model trained? -- crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
